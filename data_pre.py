import os
import pickle
import csv
import ipaddress
import numpy as np
from tqdm import tqdm
from collections import defaultdict

# ==============================================================================
# é…ç½®åŒºåŸŸ
# ==============================================================================
INPUT_PICKLE_PATH = "../../ALL_Flow_Adjust_DTW/Aright1_traffic_features_total_signed.pkl"
OUTPUT_PICKLE_PATH = "Preprocessed/Traffic_Cleaned_v2.pkl"
DEVICE_TYPE_CSV_PATH = "../../Input/device_type.csv"

# 2. Minimum Length Constraint
MIN_PACKET_COUNT = 5

# 3. Protocol Filtering (DNS List)
KNOWN_PUBLIC_DNS_IPS = {
    '8.8.8.8', '8.8.4.4', '1.1.1.1', '1.0.0.1',
    '208.67.222.222', '208.67.220.220', '9.9.9.9',
    '114.114.114.114', '114.114.115.115', '223.5.5.5', '223.6.6.6'
}

# 5. Anomaly Removal Thresholds
MAX_RST_RATIO = 0.40  # > 40% RST packets
MAX_RETRANS_RATIO = 0.50  # > 50% Retransmission


# ==============================================================================
# å·¥å…·å‡½æ•°
# ==============================================================================

def load_device_type_map(csv_path):
    """åŠ è½½è®¾å¤‡ç±»åž‹æ˜ å°„ï¼Œç”¨äºŽæŽ’é™¤ Hub"""
    mapping = {}
    if not os.path.exists(csv_path):
        print(f"âš ï¸ Warning: Device type CSV not found at {csv_path}")
        return mapping
    try:
        with open(csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            # å¤„ç† BOM å¤´
            if reader.fieldnames and reader.fieldnames[0].startswith('\ufeff'):
                reader.fieldnames[0] = reader.fieldnames[0].replace('\ufeff', '')

            if 'Device_Name' in reader.fieldnames and 'Type' in reader.fieldnames:
                for row in reader:
                    mapping[row['Device_Name'].strip()] = row['Type'].strip()
    except Exception as e:
        print(f"âŒ Error loading device map: {e}")
    return mapping


def is_private_ip(ip_str):
    """1. LAN Traffic Exclusion Check"""
    if not ip_str: return False
    try:
        # ç§»é™¤ç«¯å£å·ï¼ˆå¦‚æžœæœ‰ï¼‰
        clean_ip = ip_str.split(':')[0]
        ip_obj = ipaddress.ip_address(clean_ip)
        return ip_obj.is_private
    except ValueError:
        return False


def is_dns_traffic(remote_ip, protocol):
    """3. Protocol Filtering (DNS) Check"""
    if protocol != 'UDP':
        return False
    clean_ip = remote_ip.split(':')[0]
    if clean_ip in KNOWN_PUBLIC_DNS_IPS:
        return True
    return False


def is_tcp_anomaly(flow):
    """5. Anomaly Removal Check"""
    # å¦‚æžœä¸æ˜¯ TCPï¼Œåˆ™ä¸ç®—å¼‚å¸¸ï¼ˆæœ¬è§„åˆ™åªé’ˆå¯¹ TCPï¼‰
    if flow.get('Protocol') != 'TCP':
        return False

    # èŽ·å–æ€»åŒ…æ•°ï¼Œé˜²æ­¢é™¤ä»¥é›¶
    # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ Payload_Sequence é•¿åº¦è¿‘ä¼¼ç­‰äºŽåŒ…æ•°é‡ï¼Œæˆ–è€…æ•°æ®ä¸­æœ‰ 'Total_Packets' å­—æ®µ
    seq = flow.get('Payload_Sequence', [])
    total_packets = len(seq)
    if total_packets == 0:
        return True  # ç©ºæµè§†ä¸ºå¼‚å¸¸

    # --- èŽ·å–å…³é”®æŒ‡æ ‡ (éœ€è¦æ ¹æ®å®žé™…æ•°æ®å­—æ®µåç§°ä¿®æ”¹) ---
    # å‡è®¾æµæ•°æ®ä¸­åŒ…å« 'RST_Count' å’Œ 'Retrans_Count'
    # å¦‚æžœä½ çš„æ•°æ®åªæœ‰ flags åˆ—è¡¨ï¼Œéœ€è¦å…ˆç»Ÿè®¡
    rst_count = flow.get('RST_Count', 0)
    retrans_count = flow.get('Retrans_Count', 0)

    # æ£€æŸ¥ RST æ¯”ä¾‹
    if (rst_count / total_packets) > MAX_RST_RATIO:
        return True

    # æ£€æŸ¥é‡ä¼ æ¯”ä¾‹
    if (retrans_count / total_packets) > MAX_RETRANS_RATIO:
        return True

    return False


# ==============================================================================
# ä¸»å¤„ç†é€»è¾‘
# ==============================================================================

def main():
    print(f"ðŸš€ Starting Preprocessing...")

    # 1. åŠ è½½æ•°æ®
    if not os.path.exists(INPUT_PICKLE_PATH):
        print(f"âŒ Input file not found: {INPUT_PICKLE_PATH}")
        return

    print("â³ Loading pickle file...")
    with open(INPUT_PICKLE_PATH, 'rb') as f:
        raw_data = pickle.load(f)

    # å…¼å®¹å¤„ç†ï¼šå¦‚æžœæ˜¯å­—å…¸åˆ™æ‹å¹³
    all_flows = []
    if isinstance(raw_data, dict):
        for v in raw_data.values():
            if isinstance(v, list):
                all_flows.extend(v)
            else:
                all_flows.append(v)
    elif isinstance(raw_data, list):
        all_flows = raw_data

    print(f"ðŸ“Š Total raw flows: {len(all_flows)}")

    # åŠ è½½è®¾å¤‡æ˜ å°„
    device_type_map = load_device_type_map(DEVICE_TYPE_CSV_PATH)

    # ç»Ÿè®¡è®¡æ•°å™¨
    stats = {
        'kept': 0,
        'drop_lan': 0,  # è§„åˆ™ 1
        'drop_len': 0,  # è§„åˆ™ 2
        'drop_dns': 0,  # è§„åˆ™ 3
        'drop_hub': 0,  # è§„åˆ™ 4
        'drop_anomaly': 0  # è§„åˆ™ 5
    }

    cleaned_flows = []

    # 2. éåŽ†è¿‡æ»¤
    for flow in tqdm(all_flows, desc="Filtering Flows"):
        # æå–åŸºç¡€ä¿¡æ¯
        remote_ip = flow.get('Remote_IP', '')
        protocol = flow.get('Protocol', '')
        device_name = flow.get('Device', 'Unknown')
        seq = flow.get('Payload_Sequence', [])

        # --- Rule 4: Device Filtering (Smart Hubs) ---
        # ä¼˜å…ˆæ£€æŸ¥ï¼Œå› ä¸ºæŸ¥è¡¨å¾ˆå¿«
        dev_type = device_type_map.get(device_name, 'Unknown')
        if dev_type == 'Hub':
            stats['drop_hub'] += 1
            continue

        # --- Rule 2: Minimum Length Constraint ---
        up_cnt = sum(1 for x in seq if x > 0)
        down_cnt = sum(1 for x in seq if x < 0)
        if up_cnt < MIN_PACKET_COUNT or down_cnt < MIN_PACKET_COUNT:
            stats['drop_len'] += 1
            continue

        # --- Rule 1: LAN Traffic Exclusion ---
        if is_private_ip(remote_ip):
            stats['drop_lan'] += 1
            continue

        # --- Rule 3: Protocol Filtering (DNS) ---
        if is_dns_traffic(remote_ip, protocol):
            stats['drop_dns'] += 1
            continue

        # --- Rule 5: Anomaly Removal (TCP RST/Retrans) ---
        # if is_tcp_anomaly(flow):
        #     stats['drop_anomaly'] += 1
        #     continue

        # âœ… Passed all checks
        cleaned_flows.append(flow)
        stats['kept'] += 1

    # 3. ä¿å­˜ç»“æžœ
    print("\n" + "=" * 40)
    print("ðŸ“‹ Filter Statistics:")
    print(f"  Total Input   : {len(all_flows)}")
    print(f"  Kept          : {stats['kept']}")
    print("-" * 20)
    print(f"  âŒ Dropped (LAN)     : {stats['drop_lan']}")
    print(f"  âŒ Dropped (Min Len) : {stats['drop_len']}")
    print(f"  âŒ Dropped (DNS)     : {stats['drop_dns']}")
    print(f"  âŒ Dropped (Hubs)    : {stats['drop_hub']}")
    print(f"  âŒ Dropped (Anomaly) : {stats['drop_anomaly']}")
    print("=" * 40)

    os.makedirs(os.path.dirname(OUTPUT_PICKLE_PATH), exist_ok=True)
    with open(OUTPUT_PICKLE_PATH, 'wb') as f:
        pickle.dump(cleaned_flows, f)

    print(f"âœ… Cleaned data saved to: {OUTPUT_PICKLE_PATH}")


if __name__ == "__main__":
    main()