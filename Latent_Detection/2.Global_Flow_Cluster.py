import os
import pickle
import numpy as np
import pandas as pd
import gc
import resource
from collections import defaultdict, Counter
from scipy.cluster.hierarchy import linkage, fcluster

# ==============================================================================
# 0. Numba ç¯å¢ƒé…ç½®
# ==============================================================================
try:
    from numba import jit, prange
    HAS_NUMBA = True
    print("âœ… Numba detected! High-speed acceleration enabled.")
except ImportError:
    HAS_NUMBA = False
    print("âš ï¸ Numba not found. Install via 'pip install numba'")

    def jit(nopython=True, parallel=False):
        def decorator(func):
            return func
        return decorator

    def prange(n):
        return range(n)

# ==============================================================================
# 1. é…ç½®å‚æ•°
# ==============================================================================
BASE_DIR = "./"
INPUT_DIR = "../Data"

DTW_SIGMA = 35.0
SIMILARITY_THRESHOLD = 0.90
DISTANCE_THRESHOLD = 1.0 - SIMILARITY_THRESHOLD

# [MOD] å…¨å±€èšç±»ï¼šåºåˆ— capï¼ˆNone=ä¸é™åˆ¶ï¼›int=up/down åˆ†åˆ«æœ€å¤šå–å‰ MAX_SEQ_LEN ä¸ªï¼‰
MAX_SEQ_LEN = 100

# è¾“å…¥ï¼šæ¥è‡ª v3Final çš„â€œè®¾å¤‡å†…èšç±»ä»£è¡¨æµâ€ç»“æœï¼ˆä½ å‰ä¸€æ­¥ç”Ÿæˆçš„ï¼‰
INPUT_PKL_PATH = os.path.join(
    BASE_DIR,
    f"1.In_Device_Complete_from_v3Final_{DTW_SIGMA}_{SIMILARITY_THRESHOLD:.2f}_nopad.pkl"
)

# å¤–éƒ¨å±æ€§è¡¨
EXTERNAL_DEVICE_LIST = os.path.join(INPUT_DIR, "device_list.csv")
EXTERNAL_DEVICE_TYPE = os.path.join(INPUT_DIR, "device_type.csv")

# è¾“å‡º CSV
OUTPUT_FINAL_CSV = os.path.join(
    BASE_DIR,
    f"2.Global_Component_from_v3Final_vendor_type_{DTW_SIGMA}_{SIMILARITY_THRESHOLD:.2f}_cap{MAX_SEQ_LEN}_nopad.csv"
)

# condensed è·ç¦»çŸ©é˜µç¼“å­˜ï¼ˆé˜²æ­¢é‡å¤è®¡ç®—ï¼‰
MATRIX_CACHE_FILE = os.path.join(
    BASE_DIR,
    f"1.1.Global_Full_Distance_Matrix_Complete_{DTW_SIGMA}_{SIMILARITY_THRESHOLD:.2f}_cap{MAX_SEQ_LEN}_nopad_from_v3Final.npy"
)

# DTW å‚æ•°ï¼ˆ1.9.1 é£æ ¼ï¼‰
DTW_SMALL_THRESH = 100.0
DTW_SMALL_WEIGHT = 10.0
DTW_MTU_THRESH = 1400.0
DTW_MTU_WEIGHT = 1.0
DTW_BIG_THRESH = 350.0
DTW_BIG_WEIGHT = 8.0
DTW_CROSS_WEIGHT = 3.0

# ==============================================================================
# 2. å·¥å…·å‡½æ•°
# ==============================================================================

def limit_memory(maxsize_gb: int):
    try:
        soft, hard = resource.getrlimit(resource.RLIMIT_AS)
        maxsize_bytes = int(maxsize_gb) * 1024 * 1024 * 1024
        resource.setrlimit(resource.RLIMIT_AS, (maxsize_bytes, hard))
    except Exception:
        pass


def safe_load_pickle(path):
    """
    å…¼å®¹ list/dict/df
    è¿”å› list[dict]
    """
    try:
        obj = pd.read_pickle(path)
    except Exception:
        with open(path, "rb") as f:
            obj = pickle.load(f)

    if isinstance(obj, list):
        return obj
    if isinstance(obj, dict):
        tmp = []
        for v in obj.values():
            if isinstance(v, list):
                tmp.extend(v)
            else:
                tmp.append(v)
        return tmp
    if hasattr(obj, "to_dict"):
        return obj.to_dict(orient="records")

    raise ValueError("Unsupported pickle content format.")


def get_best_domain_name(flow_dict):
    """
    å…œåº•åŸŸåå±•ç¤ºå­—æ®µï¼šä»å¤šä¸ªå¯èƒ½çš„ key ä¸­å–ä¸€ä¸ªå¯è¯»åŸŸå
    æ³¨æ„ï¼šè¿™ä¸æ˜¯ Domain_A / Domain_PTRï¼Œåªæ˜¯ç”¨äºå±•ç¤º/è¡¥å……
    """
    potential_keys = [
        'Domain_A', 'Domain_PTR',  # ä¼˜å…ˆä½¿ç”¨ä½  v3Final çš„ç»“æœï¼ˆè‹¥å­˜åœ¨ï¼‰
        'Remote_Hostname', 'SNI', 'Server_Name', 'HTTP_Host', 'Domain', 'dns_query', 'host'
    ]
    for key in potential_keys:
        val = flow_dict.get(key)
        if val and isinstance(val, str) and len(val.strip()) > 0:
            if any(c.isalpha() for c in val):
                return val.strip()
    return ""


def extract_features_flat(signed_seq, max_seq_len=None):
    """
    å®Œå…¨æ—  padding çš„ç‰¹å¾æå–ï¼š
    - up/down åˆ†åˆ«æå– abs å€¼å¹¶ä¿æŒåŸé¡ºåº
    - å¯é€‰ capï¼šup/down å„è‡ªæœ€å¤šå–å‰ max_seq_len ä¸ª
    """
    if not signed_seq:
        return np.array([], dtype=np.float64), np.array([], dtype=np.float64)

    if max_seq_len is None:
        up = np.array([abs(x) for x in signed_seq if x > 0], dtype=np.float64)
        down = np.array([abs(x) for x in signed_seq if x < 0], dtype=np.float64)
        return up, down

    up_list = []
    down_list = []
    for x in signed_seq:
        if x > 0:
            if len(up_list) < max_seq_len:
                up_list.append(abs(x))
        elif x < 0:
            if len(down_list) < max_seq_len:
                down_list.append(abs(x))
        if len(up_list) >= max_seq_len and len(down_list) >= max_seq_len:
            break

    return np.array(up_list, dtype=np.float64), np.array(down_list, dtype=np.float64)


def pack_variable_length_sequences(feats_list):
    """
    å˜é•¿åºåˆ—æ‰“åŒ…ï¼šflat + offsets + lengthsï¼ˆå®Œå…¨æ—  paddingï¼‰
    """
    n = len(feats_list)
    lengths = np.empty(n, dtype=np.int32)
    offsets = np.empty(n, dtype=np.int64)

    total = 0
    for i, arr in enumerate(feats_list):
        l = int(arr.size)
        lengths[i] = l
        offsets[i] = total
        total += l

    flat = np.empty(total, dtype=np.float64)
    pos = 0
    for arr in feats_list:
        l = int(arr.size)
        if l > 0:
            flat[pos:pos + l] = arr
        pos += l

    return flat, offsets, lengths


# ==============================================================================
# 3. DTW æ ¸å¿ƒè®¡ç®—ï¼ˆNumbaï¼‰
# ==============================================================================

@jit(nopython=True)
def calculate_weighted_dtw_numba(
        seq_a, seq_b,
        small_thresh, small_weight,
        mtu_thresh, mtu_weight,
        big_thresh, big_weight,
        cross_weight
):
    n = len(seq_a)
    m = len(seq_b)
    if n == 0 and m == 0:
        return 0.0
    if n == 0 or m == 0:
        return 1e15

    prev_row = np.empty(m + 1, dtype=np.float64)
    curr_row = np.empty(m + 1, dtype=np.float64)
    prev_row[:] = 1e15
    prev_row[0] = 0.0

    for i in range(1, n + 1):
        curr_row[:] = 1e15
        val_a = seq_a[i - 1]
        is_small_a = val_a < small_thresh
        is_big_a = val_a > big_thresh
        is_mtu_a = val_a > mtu_thresh

        for j in range(1, m + 1):
            val_b = seq_b[j - 1]
            base_diff = abs(val_a - val_b)
            is_small_b = val_b < small_thresh
            is_big_b = val_b > big_thresh
            is_mtu_b = val_b > mtu_thresh

            if is_small_a and is_small_b:
                cost = base_diff * small_weight
            elif is_mtu_a and is_mtu_b:
                cost = base_diff / mtu_weight
            elif is_big_a and is_big_b:
                cost = base_diff / big_weight
            elif is_small_a != is_small_b:
                cost = base_diff * cross_weight
            else:
                cost = base_diff

            v1 = prev_row[j - 1]
            v2 = prev_row[j]
            v3 = curr_row[j - 1]
            min_val = v1
            if v2 < min_val:
                min_val = v2
            if v3 < min_val:
                min_val = v3
            curr_row[j] = cost + min_val

        for k in range(m + 1):
            prev_row[k] = curr_row[k]

    return prev_row[m]


@jit(nopython=True, parallel=True)
def compute_full_condensed_matrix_numba_varlen(
        flat_up, offsets_up, lengths_up,
        flat_down, offsets_down, lengths_down,
        n,
        small_thresh, small_weight, mtu_thresh, mtu_weight,
        big_thresh, big_weight, cross_weight, sigma
):
    """
    å®Œå…¨æ—  padding çš„ condensed è·ç¦»çŸ©é˜µè®¡ç®—
    """
    dist_len = n * (n - 1) // 2
    dist_array = np.empty(dist_len, dtype=np.float64)

    for i in prange(n):
        len_u_a = lengths_up[i]
        len_d_a = lengths_down[i]
        off_u_a = offsets_up[i]
        off_d_a = offsets_down[i]

        up_a = flat_up[off_u_a:off_u_a + len_u_a]
        down_a = flat_down[off_d_a:off_d_a + len_d_a]

        row_offset = i * n - (i * (i + 1)) // 2

        for j in range(i + 1, n):
            len_u_b = lengths_up[j]
            len_d_b = lengths_down[j]
            off_u_b = offsets_up[j]
            off_d_b = offsets_down[j]

            up_b = flat_up[off_u_b:off_u_b + len_u_b]
            down_b = flat_down[off_d_b:off_d_b + len_d_b]

            dist_up = calculate_weighted_dtw_numba(
                up_a, up_b,
                small_thresh, small_weight,
                mtu_thresh, mtu_weight,
                big_thresh, big_weight,
                cross_weight
            )
            dist_down = calculate_weighted_dtw_numba(
                down_a, down_b,
                small_thresh, small_weight,
                mtu_thresh, mtu_weight,
                big_thresh, big_weight,
                cross_weight
            )

            mean_len_up = (len_u_a + len_u_b) / 2.0
            mean_len_down = (len_d_a + len_d_b) / 2.0

            norm_up = dist_up / mean_len_up if mean_len_up > 0 else 1e9
            norm_down = dist_down / mean_len_down if mean_len_down > 0 else 1e9

            sim_up = np.exp(-norm_up / sigma) if mean_len_up > 0 else 0.0
            sim_down = np.exp(-norm_down / sigma) if mean_len_down > 0 else 0.0

            final_sim = np.sqrt(sim_up * sim_down)
            d = 1.0 - final_sim
            if d < 0.0:
                d = 0.0

            k = row_offset + (j - i - 1)
            dist_array[k] = d

    return dist_array


# ==============================================================================
# 4. Step 1ï¼šå…¨å±€ç»„ä»¶èšç±»ï¼ˆComplete linkageï¼‰
# ==============================================================================

def step_1_global_component_clustering(flow_data, matrix_cache_path):
    """
    è¾“å…¥ï¼šä»£è¡¨æµåˆ—è¡¨ flow_data (list[dict])
    è¾“å‡ºï¼šdf_detailsï¼ˆæ¯æ¡ä»£è¡¨æµä¸€è¡Œï¼ŒåŒ…å«å…¨å±€ Component_ID ç­‰å­—æ®µï¼‰
    """
    print("\nğŸ”µ [Step 1] Global DTW Matrix + Complete Linkage Clustering...")

    n_samples = len(flow_data)
    print(f"   Samples: {n_samples}")

    # ç‰¹å¾æå–ï¼ˆcapï¼‰+ å˜é•¿æ‰“åŒ…ï¼ˆæ—  paddingï¼‰
    feats_up_list = []
    feats_down_list = []
    for flow in flow_data:
        seq = flow.get('Payload_Sequence', [])
        u, d = extract_features_flat(seq, max_seq_len=MAX_SEQ_LEN)
        feats_up_list.append(u)
        feats_down_list.append(d)

    flat_up, offsets_up, lengths_up = pack_variable_length_sequences(feats_up_list)
    flat_down, offsets_down, lengths_down = pack_variable_length_sequences(feats_down_list)

    # condensed è·ç¦»çŸ©é˜µï¼ˆcacheï¼‰
    expected_len = n_samples * (n_samples - 1) // 2
    dist_matrix = None

    if os.path.exists(matrix_cache_path):
        print(f"   ğŸš€ Loading cached matrix: {matrix_cache_path}")
        dist_matrix = np.load(matrix_cache_path)
        if len(dist_matrix) != expected_len:
            print("   âš ï¸ Cache mismatch, recomputing...")
            dist_matrix = None

    if dist_matrix is None:
        print("   ğŸ§® Computing DTW condensed matrix (Numba, varlen, no padding)...")
        dist_matrix = compute_full_condensed_matrix_numba_varlen(
            flat_up, offsets_up, lengths_up,
            flat_down, offsets_down, lengths_down,
            n_samples,
            DTW_SMALL_THRESH, DTW_SMALL_WEIGHT, DTW_MTU_THRESH, DTW_MTU_WEIGHT,
            DTW_BIG_THRESH, DTW_BIG_WEIGHT, DTW_CROSS_WEIGHT, DTW_SIGMA
        )
        os.makedirs(os.path.dirname(matrix_cache_path) or ".", exist_ok=True)
        np.save(matrix_cache_path, dist_matrix)

    # complete linkage èšç±»
    print("   ğŸŒ² Hierarchical Clustering (Complete)...")
    Z = linkage(dist_matrix, method='complete')
    cluster_labels = fcluster(Z, t=DISTANCE_THRESHOLD, criterion='distance')

    # æ„å»º df_detailsï¼ˆä¿ç•™ Domain_A / Domain_PTRï¼‰
    rows = []
    for idx, lab in enumerate(cluster_labels):
        flow = flow_data[idx]
        seq = flow.get('Payload_Sequence', [])
        u, d = extract_features_flat(seq, max_seq_len=MAX_SEQ_LEN)

        rows.append({
            # ç»„ä»¶ idï¼ˆå…¨å±€ DTW èšç±»ï¼‰
            "Component_ID": f"Comp_{int(lab):05d}",

            # ä»£è¡¨æµçš„è®¾å¤‡ä¿¡æ¯
            "Device": str(flow.get("Device", "Unknown")),

            # è®¾å¤‡ä¾§ vendor/typeï¼ˆåé¢ Step 3/4 å†è¡¥ï¼Œä½†å…ˆä¿ç•™åŸå­—æ®µä¸ä¸¢ï¼‰
            # ç½‘ç»œä¾§
            "Remote_IP": flow.get("Remote_IP", ""),
            "Remote_Port": flow.get("Remote_Port", ""),
            "Protocol": flow.get("Protocol", ""),

            # v3Final åŸŸåå­—æ®µï¼šå¼ºåˆ¶ä¿ç•™ï¼ˆå­˜åœ¨åˆ™å†™å‡ºï¼‰
            "Domain_A": flow.get("Domain_A", ""),
            "Domain_PTR": flow.get("Domain_PTR", ""),

            # ä¸€ä¸ªä¾¿äºæŸ¥çœ‹çš„ domain å±•ç¤ºå­—æ®µï¼ˆå…œåº•ï¼‰
            "Remote_domain_best": get_best_domain_name(flow),

            # DTW åºåˆ—ï¼ˆcap åç”¨äºè§£é‡Šï¼‰
            "Sequence_Up": str(u.tolist()),
            "Sequence_Down": str(d.tolist()),

            # å›æº¯ç´¢å¼•
            "Original_Index": idx
        })

    df_details = pd.DataFrame(rows)
    return df_details


# ==============================================================================
# 5. Step 3ï¼šè¡¥å…… vendor/type ç»„ä»¶çº§å±æ€§
# ==============================================================================

def step_3_enrich_component_vendor_type(df_details, device_list_path, device_type_path):
    """
    è¾“å‡ºï¼š
      df_components: ç»„ä»¶çº§è¡¨ï¼ˆæ¯ä¸ª Component_ID ä¸€è¡Œï¼‰
      device_to_vendor, device_to_type: æ˜ å°„å­—å…¸
    """
    print("\nğŸ”µ [Step 3] Enrich Component Attributes (Vendor & Type)...")

    # è¯»å¤–éƒ¨æ˜ å°„
    if not os.path.exists(device_list_path) or not os.path.exists(device_type_path):
        print("   âš ï¸ External mapping files missing. Continue with empty mappings.")
        device_to_vendor = {}
        device_to_type = {}
    else:
        df_dev_list = pd.read_csv(device_list_path)
        df_dev_type = pd.read_csv(device_type_path)

        device_to_vendor = dict(zip(
            df_dev_list['Device_Name'].astype(str).str.strip(),
            df_dev_list['Vendor'].astype(str).str.strip()
        ))
        device_to_type = dict(zip(
            df_dev_type['Device_Name'].astype(str).str.strip(),
            df_dev_type['Type'].astype(str).str.strip()
        ))

    # ç»„ä»¶çº§èšåˆï¼šComponent_ID -> Device list
    comp_to_devices = df_details.groupby("Component_ID")["Device"].apply(list).to_dict()

    def analyze_devices(dev_list, mapping):
        """
        state=0: å…¨ä¸€è‡´ï¼›state=1: å¤šæ ·
        detail:
          - state=0: å”¯ä¸€å€¼
          - state=1: è®¡æ•°å­—ç¬¦ä¸²ï¼Œå¦‚ '3Google,1Amazon'
        """
        if not dev_list:
            return 0, ""

        attrs = []
        for d in dev_list:
            v = mapping.get(str(d).strip())
            if v and str(v).strip():
                attrs.append(str(v).strip())

        if not attrs:
            return 0, ""

        counts = Counter(attrs)
        if len(counts) == 1:
            return 0, next(iter(counts.keys()))
        else:
            sorted_attrs = sorted(counts.items(), key=lambda x: x[1], reverse=True)
            detail = ",".join([f"{c}{a}" for a, c in sorted_attrs])
            return 1, detail

    comp_rows = []
    for comp_id, devs in comp_to_devices.items():
        devs_unique = sorted(list(set([str(x) for x in devs])))

        vendor_state, vendor_detail = analyze_devices(devs_unique, device_to_vendor)
        type_state, type_detail = analyze_devices(devs_unique, device_to_type)

        comp_rows.append({
            "Component_ID": comp_id,
            "Device_Count": len(devs_unique),
            "Device_List": ",".join(devs_unique),

            "vendor_state": vendor_state,
            "vendor_detail": vendor_detail,

            "device_type_state": type_state,
            "device_type_detail": type_detail,
        })

    df_components = pd.DataFrame(comp_rows)
    return df_components, device_to_vendor, device_to_type


# ==============================================================================
# 6. Step 4ï¼šç»„ä»¶çº§ä¿¡æ¯å›çŒåˆ°æµçº§ï¼Œå¹¶å¯¼å‡º CSV
# ==============================================================================

def step_4_merge_and_export(df_components, df_details, device_to_vendor, device_to_type, output_path):
    print("\nğŸ”µ [Step 4] Merge Component-level Info Back to Flow-level & Export CSV...")

    # å›çŒï¼šç»„ä»¶çº§ -> è¡Œçº§
    df_merged = pd.merge(df_details, df_components, on="Component_ID", how="left")

    # è¡Œçº§ï¼šè®¾å¤‡ vendor/type
    df_merged["Device_Clean"] = df_merged["Device"].astype(str).str.strip()
    df_merged["device_vendor"] = df_merged["Device_Clean"].map(device_to_vendor).fillna("")
    df_merged["device_type"] = df_merged["Device_Clean"].map(device_to_type).fillna("")

    # é€‰æ‹©è¾“å‡ºåˆ—ï¼ˆæŒ‰ä½ éœ€æ±‚æœ€æ ¸å¿ƒçš„ï¼‰
    target_columns = [
        # ç»„ä»¶çº§
        "Component_ID", "Device_Count", "Device_List",
        "vendor_state", "vendor_detail",
        "device_type_state", "device_type_detail",

        # è¡Œçº§ï¼ˆä»£è¡¨æµï¼‰
        "Device", "device_vendor", "device_type",
        "Remote_IP", "Remote_Port", "Protocol",
        "Domain_A", "Domain_PTR", "Remote_domain_best",
        "Sequence_Up", "Sequence_Down",
        "Original_Index",
    ]
    available = [c for c in target_columns if c in df_merged.columns]
    df_out = df_merged[available]

    os.makedirs(os.path.dirname(output_path) or ".", exist_ok=True)
    df_out.to_csv(output_path, index=False, encoding="utf-8-sig")
    print(f"âœ… CSV saved: {output_path}")
    print(f"   Rows: {len(df_out)}, Components: {df_out['Component_ID'].nunique()}")


# ==============================================================================
# Main
# ==============================================================================

def main():
    # å¯é€‰ï¼šé™åˆ¶å†…å­˜
    try:
        resource.setrlimit(resource.RLIMIT_AS, (resource.RLIM_INFINITY, resource.RLIM_INFINITY))
    except Exception:
        pass

    if not os.path.exists(INPUT_PKL_PATH):
        print(f"âŒ Input file not found: {INPUT_PKL_PATH}")
        return

    print(f"â³ Loading input: {INPUT_PKL_PATH}")
    flow_data = safe_load_pickle(INPUT_PKL_PATH)
    print(f"ğŸ“Š Loaded flows: {len(flow_data)}")

    # Step 1ï¼šå…¨å±€ç»„ä»¶èšç±»
    df_details = step_1_global_component_clustering(flow_data, MATRIX_CACHE_FILE)
    gc.collect()

    # Step 3ï¼šè¡¥å…… vendor/type ç»„ä»¶çº§å±æ€§
    df_components, device_to_vendor, device_to_type = step_3_enrich_component_vendor_type(
        df_details, EXTERNAL_DEVICE_LIST, EXTERNAL_DEVICE_TYPE
    )
    gc.collect()

    # Step 4ï¼šå›çŒå¹¶å¯¼å‡º
    step_4_merge_and_export(df_components, df_details, device_to_vendor, device_to_type, OUTPUT_FINAL_CSV)


if __name__ == "__main__":
    main()
